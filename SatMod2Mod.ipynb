{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Mod2Mod\n\nWe explore using deep learning to learn mappings from one satellite imaging modality to another.\n\n* Some modalitities are expensive to obtain, or not available at the needed time.\n* Some modalitities are more intrerpretable to the untrained eye\n* Some algorithms require having specific bands, or they are trained on specific bands and we would like to achieve a similar performance on new bands\n\nThis technique is called Colorification (used for converting grayscale to RGB images, but here we consider the more general case of mapping). It can be achieved by fitting a function from one modality to the other one and when a lot of data is available can be tackled by deep learning.\n\nhttps://arxiv.org/pdf/1604.02245.pdf\n\nhttps://richzhang.github.io/colorization/\n\n\nWe will explore several modalities: RGB, Infrared, Radar."},{"metadata":{},"cell_type":"markdown","source":"### RGB2IR"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will use a small image from Planet Labs RGB-IR image to test the pipeline."},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install scikit-image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# read the image\nimport skimage\n\nim = skimage.io.imread('../input/planet-image/1057916_2016-07-07_RE1_3A_Visual_clip.tif')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"im = im[:1536,:1536,:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.imshow(im[:,:,7],cmap = 'gray')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reading an sentinel1&sentinel2 image combo extracted from Google Earth Engine\n#im = skimage.io.imread('../input/ls-yakima/LS_Yakima_20180811.tif')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(im[:,:,3],cmap = 'gray')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from skimage.util.shape import view_as_blocks\nim_blocks = view_as_blocks(im, block_shape=(128, 128, 4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting into tiles\nim_blocks.shape\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e61ef2d8-f315-4f7f-b07e-1de0f4e8441a","_uuid":"1677fddbb95f7545b6540e9201f3339a0fdbfc5d"},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":"![](http://)We will use Keras and a deep learning architecture called [U-net](https://arxiv.org/abs/1505.04597) (which has been successful in many imaging domains)."},{"metadata":{"_cell_guid":"c332549b-8d23-4bb5-8497-e7a8eb8b21d2","_uuid":"5c38504af3a84bee68c66d3cde74443c58df422f","trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom itertools import chain\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label\n\nfrom keras.models import Model, load_model\nfrom keras.layers import Input\nfrom keras.layers.core import Dropout, Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import backend as K\n\nimport tensorflow as tf\n\n# Set some parameters\nIMG_WIDTH = 128\nIMG_HEIGHT = 128\nIMG_CHANNELS = 3\nTRAIN_PATH = '../input/stage1_train/'\nTEST_PATH = '../input/stage1_test/'\n\nwarnings.filterwarnings('ignore', category=UserWarning, module='skimage')\nseed = 42\nrandom.seed = seed\nnp.random.seed = seed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.imshow(im[:,:,3],cmap = 'gray')\nplt.hist(im[:,:,3].ravel(),2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imsize = 128","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(im[:,:,:3])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"im_blocks.squeeze().shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"im_blocks = im_blocks.squeeze().reshape((12*12,128,128,4))\nim_blocks.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train = im_blocks[:8,:,:,:3] \n#Y_train = im_blocks[:8,:,:,-1]\n#X_test = im_blocks[8:,:,:,:3]\n#Y_test = im_blocks[8:,:,:,-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"im_blocks.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"im_blocks[:,:,:,3].reshape(dim[0],dim[1],dim[2],1).shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we will randomly split\nX_train, X_test, Y_train, Y_test = train_test_split(im_blocks[:,:,:,:3], im_blocks[:,:,:,:3].mean(axis=3), test_size=0.33, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_train.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ffa0caf0-2d1b-40f2-865b-8e6db88526b6","_uuid":"3fb9d6530fbbd0e22e41fc4fd9fd9fc0bff027ac","trusted":true},"cell_type":"code","source":"Y_train = Y_train.reshape(Y_train.shape[0],128,128,1)\nY_test = Y_test.reshape(Y_test.shape[0],128,128,1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"59c4a25d-645f-4b74-9c53-145ac78cc481","_uuid":"875af74f980236825de3a650825b46e25632422c"},"cell_type":"markdown","source":""},{"metadata":{"_cell_guid":"c0523b03-1fc5-4505-a1b8-eb35ee617c8a","_uuid":"d4f8327802a1ec6139ce0585953986272ba62ce1"},"cell_type":"markdown","source":"Let's see if things look all right by drawing some random images and their associated masks."},{"metadata":{"_cell_guid":"ca0cc34b-c26f-41ee-88d7-975aebdb634e","_uuid":"9e389ba8bdb5b6fc03b231b6a6c84a8bde634053","trusted":true},"cell_type":"markdown","source":"# Get and resize train images and masks\nX_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nY_train = np.zeros((len(train_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\nprint('Getting and resizing train images and masks ... ')\nsys.stdout.flush()\nfor n, id_ in tqdm(enumerate(train_ids), total=len(train_ids)):\n    path = TRAIN_PATH + id_\n    img = imread(path + '/images/' + id_ + '.png')[:,:,:IMG_CHANNELS]\n    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n    X_train[n] = img\n    mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n    for mask_file in next(os.walk(path + '/masks/'))[2]:\n        mask_ = imread(path + '/masks/' + mask_file)\n        mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant', \n                                      preserve_range=True), axis=-1)\n        mask = np.maximum(mask, mask_)\n    Y_train[n] = mask\n\n# Get and resize test images\nX_test = np.zeros((len(test_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\nsizes_test = []\nprint('Getting and resizing test images ... ')\nsys.stdout.flush()\nfor n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):\n    path = TEST_PATH + id_\n    img = imread(path + '/images/' + id_ + '.png')[:,:,:IMG_CHANNELS]\n    sizes_test.append([img.shape[0], img.shape[1]])\n    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n    X_test[n] = img\n\nprint('Done!')"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Y_train = Y_train.reshape(Y_train.shape[0]*Y_train.shape[1],imsize,imsize,1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train = X_train.reshape((12*12,128,128,3))\n#print(X_train.shape)\n#Y_train = Y_train.reshape((12*12,128,128,1))\n#print(Y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_test.min()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"88829b53-50ce-45d9-9540-77dd7384ad4c","_uuid":"283af26f0860b7069bdfd133c746e5d20971542c","trusted":true},"cell_type":"code","source":"# Check if training data looks all right\nix = random.randint(0, len(im_blocks))\nplt.imshow(X_train[0,:,:,0],)\nplt.imshow(Y_train[0,:,:,0],cmap = 'gray')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2574ffe9-b911-4bfd-a00f-9ba5c25f45de","_uuid":"938648da705689a0f940ff462477c801db3f0737"},"cell_type":"markdown","source":"We are going to \n\n"},{"metadata":{"_cell_guid":"c1df6f3a-d58f-434b-9216-ef7be38637d4","_uuid":"5abd38950ae99b60f8afec7656eb654a48d449fe","trusted":true},"cell_type":"code","source":"# Define IoU metric\n# def mean_iou(y_true, y_pred):\n#    prec = []\n#    for t in np.arange(0.5, 1.0, 0.05):\n#        y_pred_ = tf.to_int32(y_pred > t)\n#        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n#        K.get_session().run(tf.local_variables_initializer())\n#        with tf.control_dependencies([up_opt]):\n#            score = tf.identity(score)\n#        prec.append(score)\n#    return K.mean(K.stack(prec), axis=0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c3b9f148-1dba-4b6a-981b-6cdbf394fc3c","_uuid":"986488a4c5223576be370e224426a30431911eb2"},"cell_type":"markdown","source":"# Build and train our neural network\nNext we build our U-Net model, loosely based on [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/pdf/1505.04597.pdf) and very similar to [this repo](https://github.com/jocicmarko/ultrasound-nerve-segmentation) from the Kaggle Ultrasound Nerve Segmentation competition.\n\n![](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)"},{"metadata":{"_cell_guid":"c1dbc57c-b497-4ccb-b077-2053203ab7ed","_uuid":"0aa97d66c29f45dfac9b0f45fcf74ba0e778ba5d","trusted":true},"cell_type":"code","source":"# Build U-Net model\ninputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\ns = Lambda(lambda x: x / 255) (inputs)\n\nc1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (s)\nc1 = Dropout(0.1) (c1)\nc1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\np1 = MaxPooling2D((2, 2)) (c1)\n\nc2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\nc2 = Dropout(0.1) (c2)\nc2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\np2 = MaxPooling2D((2, 2)) (c2)\n\nc3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\nc3 = Dropout(0.2) (c3)\nc3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\np3 = MaxPooling2D((2, 2)) (c3)\n\nc4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\nc4 = Dropout(0.2) (c4)\nc4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\np4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n\nc5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)\nc5 = Dropout(0.3) (c5)\nc5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)\n\nu6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\nu6 = concatenate([u6, c4])\nc6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\nc6 = Dropout(0.2) (c6)\nc6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n\nu7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\nu7 = concatenate([u7, c3])\nc7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\nc7 = Dropout(0.2) (c7)\nc7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n\nu8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\nu8 = concatenate([u8, c2])\nc8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)\nc8 = Dropout(0.1) (c8)\nc8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n\nu9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\nu9 = concatenate([u9, c1], axis=3)\nc9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)\nc9 = Dropout(0.1) (c9)\nc9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)\n\noutputs = Conv2D(1, (1, 1), activation='linear') (c9)\n\nmodel = Model(inputs=[inputs], outputs=[outputs])\nmodel.compile(optimizer='adam', loss='mean_squared_error')\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"72330944-6ce7-4070-b276-c3c4b20c4fe5","_uuid":"92350b6e18cc50f3fa7b6e9a02d39fcbff8238f7"},"cell_type":"markdown","source":"*Update: Changed to ELU units, added dropout.*\n\nNext we fit the model on the training data, using a validation split of 0.1. We use a small batch size because we have so little data. I recommend using checkpointing and early stopping when training your model. I won't do it here to make things a bit more reproducible (although it's very likely that your results will be different anyway). I'll just train for 10 epochs, which takes around 10 minutes in the Kaggle kernel with the current parameters. \n\n*Update: Added early stopping and checkpointing and increased to 30 epochs.*"},{"metadata":{"_cell_guid":"9415b1c4-aa69-41b9-a1e3-d6053dbd4f64","_uuid":"c060db22daa2abf12b28240cd81bbcbf1ce1bf87","trusted":true},"cell_type":"code","source":"# Fit model\nearlystopper = EarlyStopping(patience=5, verbose=1)\ncheckpointer = ModelCheckpoint('model-dsbowl2018-1.h5', verbose=1, save_best_only=True)\nresults = model.fit(X_train, Y_train, validation_split=0.1, batch_size=16, epochs=50, \n                    callbacks=[earlystopper, checkpointer])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1f381f5b-1b71-4daa-a417-e02f4894540b","_uuid":"bb15226ea617cf91ed8f43179fccb5a15809e5a0"},"cell_type":"markdown","source":"All right, looks good! Loss seems to be a bit erratic, though. I'll leave it to you to improve the model architecture and parameters! \n\n# Make predictions\n\nLet's make predictions both on the test set, the val set and the train set (as a sanity check). Remember to load the best saved model if you've used early stopping and checkpointing."},{"metadata":{"_cell_guid":"2daa48d5-ac98-4e18-af3f-a582baaa44f0","_uuid":"f841760b4abca1a25cb750822f88268bd79bf2ce","trusted":true},"cell_type":"code","source":"# Predict on train, val and test\nmodel = load_model('model-dsbowl2018-1.h5')\npreds_train = model.predict(X_train[:int(X_train.shape[0]*0.9)], verbose=1)\npreds_val = model.predict(X_train[int(X_train.shape[0]*0.9):], verbose=1)\npreds_test = model.predict(X_test, verbose=1)\n\n# Threshold predictions\npreds_train_t = (preds_train > 0.5).astype(np.uint8)\npreds_val_t = (preds_val > 0.5).astype(np.uint8)\npreds_test_t = (preds_test > 0.5).astype(np.uint8)\n\n# Create list of upsampled test masks\npreds_test_upsampled = []\nfor i in range(len(preds_test)):\n    preds_test_upsampled.append(resize(np.squeeze(preds_test[i]), \n                                       (sizes_test[i][0], sizes_test[i][1]), \n                                       mode='constant', preserve_range=True))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Perform a sanity check on some random training samples\nix = random.randint(0, X_train.shape[0])\nimshow(X_train[ix])\nplt.show()\nimshow(np.squeeze(Y_train[ix]),cmap = 'gray')\nplt.show()\nimshow(np.squeeze(preds_train[ix]),cmap = 'gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"649248cd-a1fb-4da6-ade2-4bebad44bcab","_uuid":"7e06242a50870e07a080064a4912b761775990fa","trusted":true},"cell_type":"code","source":"# Perform a sanity check on some random training samples\nix = random.randint(0, X_train.shape[0])\nimshow(X_train[ix])\nplt.show()\nimshow(np.squeeze(Y_train[ix]),cmap = 'gray')\nplt.show()\nimshow(np.squeeze(preds_train[ix]),cmap = 'gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_train.min()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"af602aea-5e56-42a8-9331-54b4b2650593","_uuid":"5fcee2b9aee2fba5c60d43ad48a14139e9c1318c"},"cell_type":"markdown","source":"The model is at least able to fit to the training data! Certainly a lot of room for improvement even here, but a decent start. How about the validation data?"},{"metadata":{"_cell_guid":"4f66b75c-c694-41a1-8c91-34bb6595837b","_uuid":"d4ccbb559375bc2777ffb692a20adc313159f2cc","trusted":true},"cell_type":"code","source":"# Perform a sanity check on some random validation samples\nix = random.randint(0, preds_val.shape[0])\nimshow(X_train[int(X_train.shape[0]*0.9):][ix])\nplt.show()\nimshow(np.squeeze(Y_train[int(Y_train.shape[0]*0.9):][ix]),cmap = 'gray')\nplt.show()\nimshow(np.squeeze(preds_val[ix]),cmap = 'gray')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a6690535-b2e4-49ac-98d9-7191bfabfb6f","_uuid":"6a34c98de7c6ae473f676a34fe7e099b46764eca"},"cell_type":"markdown","source":""},{"metadata":{"_cell_guid":"59a0af60-a7d7-41ef-a6fe-9e3c72defa07","_uuid":"4f99c1bf852e82b60bd4f982ca0df293f712cdf0","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"31133f8c-3f40-4dff-8e1d-898d56672332","_uuid":"2e07f6afc4787b068ba714428145dcb3951d718f"},"cell_type":"markdown","source":""},{"metadata":{"_cell_guid":"22fe24a1-7659-4cc9-9d23-211f38e5b99f","_uuid":"089587843ed6a3955fdcb9b23a6ec3bf5d703688","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"20b6b627-0fd6-425d-888f-da7f39efb124","_uuid":"849184a40a2c9c21506d8b8eb10ad9155fa229e8"},"cell_type":"markdown","source":""},{"metadata":{"_cell_guid":"1ba0ee3a-cca0-4349-83f6-09a1ac6fcb44","_uuid":"ba589f56f5be1e6886bc88f5bf9e7d0a408e4048","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"222475b9-3171-461a-90f0-a820a6bd2634","_uuid":"fb5e6f8cca872f1bd7036f6d9ac2ed2cab615536"},"cell_type":"markdown","source":""},{"metadata":{"_cell_guid":"3f5e5a47-6133-4870-976a-a8e4fa7bf46c","_uuid":"2a83eab66bf55194f300953bea5534b6a043130f","trusted":true},"cell_type":"code","source":"### Future:\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Future:\n    * test with an model pretrained on satellite imagery\n    * test rural vs urban\n    * "}],"metadata":{"language_info":{"mimetype":"text/x-python","version":"3.6.4","name":"python","codemirror_mode":{"version":3,"name":"ipython"},"nbconvert_exporter":"python","pygments_lexer":"ipython3","file_extension":".py"},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":1}